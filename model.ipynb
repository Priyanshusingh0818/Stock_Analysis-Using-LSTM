{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPredictor:\n",
    "    def __init__(self, symbol='AAPL', period='6mp'):\n",
    "        self.symbol = symbol\n",
    "        self.period = period\n",
    "        self.data = None\n",
    "        self.models = {}\n",
    "        self.close_scaler = RobustScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def fetch_data(self):\n",
    "        \"\"\"Fetch historical stock data using yfinance\"\"\"\n",
    "        try:\n",
    "            self.data = yf.download(self.symbol, period=self.period, interval='1d')\n",
    "            if self.data.empty:\n",
    "                self.logger.error(f\"No data retrieved for {self.symbol}\")\n",
    "                return False\n",
    "            \n",
    "            self.logger.info(f\"Successfully fetched {len(self.data)} rows for {self.symbol}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to fetch data for {self.symbol}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def add_technical_indicators(self):\n",
    "        \"\"\"Add technical indicators to the dataset\"\"\"\n",
    "        if self.data is None or self.data.empty:\n",
    "            self.logger.error(\"No data available for adding technical indicators\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            self.data['Daily_Return'] = self.data['Close'].pct_change()\n",
    "            self.data['MA50'] = self.data['Close'].rolling(window=50).mean()\n",
    "            self.data['MA200'] = self.data['Close'].rolling(window=200).mean()\n",
    "\n",
    "            # Calculate RSI\n",
    "            delta = self.data['Close'].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "            rs = gain / loss\n",
    "            self.data['RSI'] = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # Calculate Volatility\n",
    "            self.data['Volatility'] = self.data['Daily_Return'].rolling(window=20).std()\n",
    "            \n",
    "            # Drop NaN values and verify we still have data\n",
    "            initial_size = len(self.data)\n",
    "            self.data.dropna(inplace=True)\n",
    "            final_size = len(self.data)\n",
    "            \n",
    "            self.logger.info(f\"Removed {initial_size - final_size} rows with NaN values\")\n",
    "            \n",
    "            if final_size == 0:\n",
    "                self.logger.error(\"No data remaining after removing NaN values\")\n",
    "                return False\n",
    "                \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error adding technical indicators: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def prepare_data(self, look_back=60):\n",
    "        \"\"\"Prepare data for model training\"\"\"\n",
    "        if self.data is None or self.data.empty:\n",
    "            self.logger.error(\"No data available for preparation\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            self.data['Target'] = self.data['Close'].pct_change().shift(-1)\n",
    "            features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            \n",
    "            # Verify all required features exist\n",
    "            missing_features = [f for f in features if f not in self.data.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"Missing required features: {missing_features}\")\n",
    "                return None, None\n",
    "\n",
    "            X = self.data[features].values\n",
    "            y = self.data['Target'].values[:-1]  # Remove last row as it will have NaN target\n",
    "\n",
    "            if len(X) == 0 or len(y) == 0:\n",
    "                self.logger.error(\"No data available after preprocessing\")\n",
    "                return None, None\n",
    "\n",
    "            # Scale the features and target\n",
    "            X_scaled = self.feature_scaler.fit_transform(X)\n",
    "            y_scaled = self.close_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Create sequences\n",
    "            X_seq, y_seq = [], []\n",
    "            for i in range(len(X_scaled) - look_back - 1):\n",
    "                X_seq.append(X_scaled[i:(i + look_back)])\n",
    "                y_seq.append(y_scaled[i + look_back])\n",
    "            \n",
    "            X_seq = np.array(X_seq)\n",
    "            y_seq = np.array(y_seq)\n",
    "            \n",
    "            if len(X_seq) == 0 or len(y_seq) == 0:\n",
    "                self.logger.error(\"No sequences created\")\n",
    "                return None, None\n",
    "                \n",
    "            self.logger.info(f\"Created {len(X_seq)} sequences of length {look_back}\")\n",
    "            return X_seq, y_seq\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error preparing data: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def create_lstm_model(self, input_shape):\n",
    "        \"\"\"Create and compile LSTM model\"\"\"\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(256, return_sequences=True), input_shape=input_shape),\n",
    "            Dropout(0.3),\n",
    "            Bidirectional(LSTM(256, return_sequences=False)),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    def train_models(self, X, y):\n",
    "        \"\"\"Train all models\"\"\"\n",
    "        # Train LSTM\n",
    "        X_lstm = X.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "        ]\n",
    "        \n",
    "        lstm_model = self.create_lstm_model((X.shape[1], X.shape[2]))\n",
    "        lstm_history = lstm_model.fit(\n",
    "            X_lstm, y,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Train XGBoost\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8\n",
    "        )\n",
    "\n",
    "        # Train Random Forest\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=8,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Flatten data for tree-based models\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        xgb_model.fit(X_flat, y)\n",
    "        rf_model.fit(X_flat, y)\n",
    "\n",
    "        self.models = {\n",
    "            'lstm': lstm_model,\n",
    "            'xgboost': xgb_model,\n",
    "            'random_forest': rf_model\n",
    "        }\n",
    "\n",
    "        return lstm_history\n",
    "\n",
    "    def evaluate_models(self, X, y):\n",
    "        \"\"\"Evaluate all models\"\"\"\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        X_lstm = X.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "\n",
    "        predictions = {\n",
    "            'lstm': self.models['lstm'].predict(X_lstm),\n",
    "            'xgboost': self.models['xgboost'].predict(X_flat),\n",
    "            'random_forest': self.models['random_forest'].predict(X_flat)\n",
    "        }\n",
    "\n",
    "        metrics = {}\n",
    "        for name, pred in predictions.items():\n",
    "            metrics[name] = {\n",
    "                'mae': mean_absolute_error(y, pred),\n",
    "                'mse': mean_squared_error(y, pred),\n",
    "                'rmse': np.sqrt(mean_squared_error(y, pred)),\n",
    "                'r2': r2_score(y, pred)\n",
    "            }\n",
    "        return metrics\n",
    "\n",
    "    def save_models(self, output_dir='models'):\n",
    "        \"\"\"Save models and scalers to files\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        self.models['lstm'].save(f'{output_dir}/lstm_model.keras')\n",
    "        with open(f'{output_dir}/xgboost_model.pkl', 'wb') as f:\n",
    "            pickle.dump(self.models['xgboost'], f)\n",
    "        with open(f'{output_dir}/random_forest_model.pkl', 'wb') as f:\n",
    "            pickle.dump(self.models['random_forest'], f)\n",
    "        with open(f'{output_dir}/close_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.close_scaler, f)\n",
    "        with open(f'{output_dir}/feature_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.feature_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "2025-01-29 19:54:13,515 - INFO: Successfully fetched 126 rows for AAPL\n",
      "2025-01-29 19:54:13,523 - INFO: Removed 126 rows with NaN values\n",
      "2025-01-29 19:54:13,523 - ERROR: No data remaining after removing NaN values\n",
      "2025-01-29 19:54:13,526 - ERROR: No data available for preparation\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m predictor\u001b[38;5;241m.\u001b[39madd_technical_indicators()\n\u001b[0;32m      4\u001b[0m X, y \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mprepare_data(look_back\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m metrics \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mevaluate_models(X, y)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_metrics \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[48], line 129\u001b[0m, in \u001b[0;36mStockPredictor.train_models\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train all models\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Train LSTM\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m X_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    130\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    131\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    132\u001b[0m     ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m    133\u001b[0m ]\n\u001b[0;32m    135\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_lstm_model((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "predictor = StockPredictor(symbol='AAPL', period='6mo')\n",
    "if predictor.fetch_data():\n",
    "    predictor.add_technical_indicators()\n",
    "    X, y = predictor.prepare_data(look_back=60)\n",
    "    history = predictor.train_models(X, y)\n",
    "    metrics = predictor.evaluate_models(X, y)\n",
    "    \n",
    "    for model_name, model_metrics in metrics.items():\n",
    "        print(f\"\\n{model_name.upper()} Metrics:\")\n",
    "        for metric_name, value in model_metrics.items():\n",
    "            print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    predictor.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
